{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM1QuHJcDl7Wjo9A7v5M92w",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prabhatkumar13/mHAR-a-novel-convolutional-recurrent-model-for-recognizing-motion-based-human-activity/blob/main/Source_Code_for_mHAR_a_novel_convolutional_recurrent_model_for_recognizing_motion_based_human_activity.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EjVDgO5zYM46"
      },
      "outputs": [],
      "source": [
        "#YCS-2023_WISDM_mHAR_Model_Source_Code\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Flatten, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "print(tf.__version__)\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "\n",
        "file = open('/content/drive/MyDrive/WISDM/WISDM_ar_v1.1/WISDM_ar_v1.1_raw.txt')\n",
        "lines = file.readlines()\n",
        "\n",
        "processedList = []\n",
        "\n",
        "for i, line in enumerate(lines):\n",
        "    try:\n",
        "        line = line.split(',')\n",
        "        last = line[5].split(';')[0]\n",
        "        last = last.strip()\n",
        "        if last == '':\n",
        "            break;\n",
        "        temp = [line[0], line[1], line[2], line[3], line[4], last]\n",
        "        processedList.append(temp)\n",
        "    except:\n",
        "        print('Error at line number: ', i)\n",
        "\n",
        "\n",
        "columns = ['user', 'activity', 'time', 'x', 'y', 'z']\n",
        "\n",
        "data = pd.DataFrame(data = processedList, columns = columns)\n",
        "data.head()\n",
        "\n",
        "data.shape\n",
        "\n",
        "data.isnull().sum()\n",
        "\n",
        "data['x'] = data['x'].astype('float')\n",
        "data['y'] = data['y'].astype('float')\n",
        "data['z'] = data['z'].astype('float')\n",
        "\n",
        "activities = data['activity'].value_counts().index\n",
        "activities\n",
        "\n",
        "data['activity'].value_counts()\n",
        "\n",
        "# Normalize features for training data set (values between 0 and 1)\n",
        "# Surpress warning for next 3 operation\n",
        "pd.options.mode.chained_assignment = None  # default='warn'\n",
        "data['x'] = data['x'].astype(float) / data['x'].astype(float).max()\n",
        "data['y'] = data['y'].astype(float) / data['y'].astype(float).max()\n",
        "data['z'] = data['z'].astype(float) / data['z'].astype(float).max()\n",
        "# Round numbers\n",
        "data = data.round({'x': 4, 'y': 4, 'z': 4})\n",
        "\n",
        "static = data[data['activity'].isin (['Sitting', 'Standing'])]\n",
        "static\n",
        "\n",
        "static['Encode'] = \"Static\"\n",
        "\n",
        "static\n",
        "\n",
        "gradual = data[data['activity'].isin (['Upstairs', 'Downstairs'])]\n",
        "gradual\n",
        "\n",
        "gradual['Encode'] = \"Gradual\"\n",
        "gradual\n",
        "\n",
        "dynamic = data[data['activity'].isin (['Walking', 'Jogging'])]\n",
        "dynamic\n",
        "\n",
        "dynamic['Encode'] = \"Dynamic\"\n",
        "dynamic\n",
        "\n",
        "frames = [static, gradual, dynamic\n",
        "          ]\n",
        "dataCol = pd.concat(frames)\n",
        "print(dataCol)\n",
        "dataCol['Encode'].value_counts()\n",
        "\n",
        "dataCol\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "label = LabelEncoder()\n",
        "dataCol['label'] = label.fit_transform(dataCol['Encode'])\n",
        "dataCol.head()\n",
        "\n",
        "dataCol['label'].value_counts()\n",
        "\n",
        "dataCol = dataCol.drop(['user', 'activity', 'time', 'Encode'], axis = 1).copy()\n",
        "dataCol.head()\n",
        "\n",
        "label.classes_\n",
        "\n",
        "Fs = 50\n",
        "frame_size = Fs*4 # 200\n",
        "hop_size = Fs*2 # 200\n",
        "\n",
        "import math as m\n",
        "def get_frames(data, frame_size, hop_size):\n",
        "\n",
        "  N_FEATURES = 3\n",
        "  frames = []\n",
        "  labels = []\n",
        "  for i in range(0, len(dataCol) - frame_size, hop_size):\n",
        "    x = data['x'].values[i: i + frame_size]\n",
        "    y = data['y'].values[i: i + frame_size]\n",
        "    z = data['z'].values[i: i + frame_size]\n",
        "        # Retrieve the most often used label in this segment\n",
        "    label = stats.mode(dataCol['label'][i: i + frame_size])[0][0]\n",
        "    frames.append([x, y, z])\n",
        "    labels.append(label)\n",
        "  # Bring the segments into a better shape\n",
        "  frames = np.asarray(frames, dtype= np.float32).reshape(-1, frame_size, N_FEATURES)\n",
        "  labels = np.asarray(labels)\n",
        "  return frames, labels\n",
        "\n",
        "from scipy import stats\n",
        "X,y = get_frames(dataCol, frame_size, hop_size)\n",
        "\n",
        "X.shape\n",
        "y.shape\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state = 0, stratify = y)\n",
        "\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)\n",
        "\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,Dropout,BatchNormalization\n",
        "from keras.layers import Conv1D,MaxPooling1D,Flatten\n",
        "\n",
        "from keras import backend as K\n",
        "\n",
        "def recall_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "def precision_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "def f1_m(y_true, y_pred):\n",
        "    precision = precision_m(y_true, y_pred)\n",
        "    recall = recall_m(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation, Conv1D, Dropout, MaxPooling1D, Flatten, LSTM,BatchNormalization, SimpleRNN\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv1D(filters=64, kernel_size=3, padding='same', activation='relu',input_shape=x_train[0].shape))\n",
        "model.add(LSTM(30))\n",
        "model.add((Flatten()))\n",
        "model.add(Dense(3, activation=\"softmax\"))\n",
        "model.summary()\n",
        "\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "opt = Adam(learning_rate=0.0001)\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer=opt, metrics=['accuracy',f1_m,precision_m,recall_m])\n",
        "\n",
        "from time import perf_counter\n",
        "print(\"Time Counting Started :\")\n",
        "t1_start = perf_counter()\n",
        "\n",
        "score = model.fit(x_train,y_train, epochs=30, validation_data= (x_test, y_test),batch_size=64, verbose=1)\n",
        "\n",
        "t1_stop = perf_counter()\n",
        "print(\"Elapsed time:\", t1_stop, t1_start)\n",
        "print(\"Elapsed time during the whole program in seconds:\",\n",
        "                                        t1_stop-t1_start)\n",
        "\n",
        "# Evaluate the model on the test data using `evaluate`\n",
        "print(\"Evaluate on test data\")\n",
        "results = model.evaluate(x_test, y_test)\n",
        "\n",
        "predy = np.argmax(model.predict(x_test), axis=1)\n",
        "\n",
        "from mlxtend.plotting import plot_confusion_matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "cm=confusion_matrix(y_test,predy)\n",
        "print(cm)\n",
        "print(accuracy_score(y_test,predy))\n",
        "print(\"classification_report of model1:\")\n",
        "\n",
        "mat = confusion_matrix(y_test,predy)\n",
        "\n",
        "print('Precision: %.3f' % precision_score(y_test, predy, average='micro'))\n",
        "target_names = ['class 0', 'class 1', 'class 2']\n",
        "\n",
        "print(classification_report(y_test,predy, target_names=target_names))\n",
        "\n",
        "plot_confusion_matrix(conf_mat=mat, show_normed=True)\n",
        "\n",
        "plot_confusion_matrix(conf_mat=mat, show_normed=True, show_absolute=False, colorbar=True)\n",
        "\n",
        "import matplotlib.pyplot as plt1\n",
        "\n",
        "# Plot training & validation accuracy values\n",
        "plt.figure(figsize=(4,4))\n",
        "plt.plot(score.history['accuracy'],label='Train accuracy')\n",
        "plt.plot(score.history['val_accuracy'],label = 'Validation accuracy')\n",
        "plt.xlabel('epoch no')\n",
        "plt.ylabel('acc')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(4,4))\n",
        "plt.plot(score.history['loss'],label='Train loss')\n",
        "plt.plot(score.history['val_loss'],label = 'Validation loss')\n",
        "plt.xlabel('epoch no')\n",
        "plt.ylabel('loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#YCS-2023_WISDM_RNN_Model_Source_Code\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Flatten, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "print(tf.__version__)\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "\n",
        "file = open('/content/drive/MyDrive/WISDM/WISDM_ar_v1.1/WISDM_ar_v1.1_raw.txt')\n",
        "lines = file.readlines()\n",
        "\n",
        "processedList = []\n",
        "\n",
        "for i, line in enumerate(lines):\n",
        "    try:\n",
        "        line = line.split(',')\n",
        "        last = line[5].split(';')[0]\n",
        "        last = last.strip()\n",
        "        if last == '':\n",
        "            break;\n",
        "        temp = [line[0], line[1], line[2], line[3], line[4], last]\n",
        "        processedList.append(temp)\n",
        "    except:\n",
        "        print('Error at line number: ', i)\n",
        "\n",
        "\n",
        "columns = ['user', 'activity', 'time', 'x', 'y', 'z']\n",
        "\n",
        "data = pd.DataFrame(data = processedList, columns = columns)\n",
        "data.head()\n",
        "\n",
        "data.shape\n",
        "\n",
        "data.isnull().sum()\n",
        "\n",
        "data['x'] = data['x'].astype('float')\n",
        "data['y'] = data['y'].astype('float')\n",
        "data['z'] = data['z'].astype('float')\n",
        "\n",
        "activities = data['activity'].value_counts().index\n",
        "activities\n",
        "\n",
        "data['activity'].value_counts()\n",
        "\n",
        "# Normalize features for training data set (values between 0 and 1)\n",
        "# Surpress warning for next 3 operation\n",
        "pd.options.mode.chained_assignment = None  # default='warn'\n",
        "data['x'] = data['x'].astype(float) / data['x'].astype(float).max()\n",
        "data['y'] = data['y'].astype(float) / data['y'].astype(float).max()\n",
        "data['z'] = data['z'].astype(float) / data['z'].astype(float).max()\n",
        "# Round numbers\n",
        "data = data.round({'x': 4, 'y': 4, 'z': 4})\n",
        "\n",
        "static = data[data['activity'].isin (['Sitting', 'Standing'])]\n",
        "static\n",
        "\n",
        "static['Encode'] = \"Static\"\n",
        "\n",
        "static\n",
        "\n",
        "gradual = data[data['activity'].isin (['Upstairs', 'Downstairs'])]\n",
        "gradual\n",
        "\n",
        "gradual['Encode'] = \"Gradual\"\n",
        "gradual\n",
        "\n",
        "dynamic = data[data['activity'].isin (['Walking', 'Jogging'])]\n",
        "dynamic\n",
        "\n",
        "dynamic['Encode'] = \"Dynamic\"\n",
        "dynamic\n",
        "\n",
        "frames = [static, gradual, dynamic\n",
        "          ]\n",
        "dataCol = pd.concat(frames)\n",
        "print(dataCol)\n",
        "dataCol['Encode'].value_counts()\n",
        "\n",
        "dataCol\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "label = LabelEncoder()\n",
        "dataCol['label'] = label.fit_transform(dataCol['Encode'])\n",
        "dataCol.head()\n",
        "\n",
        "dataCol['label'].value_counts()\n",
        "\n",
        "dataCol = dataCol.drop(['user', 'activity', 'time', 'Encode'], axis = 1).copy()\n",
        "dataCol.head()\n",
        "\n",
        "label.classes_\n",
        "\n",
        "Fs = 50\n",
        "frame_size = Fs*4 # 200\n",
        "hop_size = Fs*2 # 200\n",
        "\n",
        "import math as m\n",
        "def get_frames(data, frame_size, hop_size):\n",
        "\n",
        "  N_FEATURES = 3\n",
        "  frames = []\n",
        "  labels = []\n",
        "  for i in range(0, len(dataCol) - frame_size, hop_size):\n",
        "    x = data['x'].values[i: i + frame_size]\n",
        "    y = data['y'].values[i: i + frame_size]\n",
        "    z = data['z'].values[i: i + frame_size]\n",
        "        # Retrieve the most often used label in this segment\n",
        "    label = stats.mode(dataCol['label'][i: i + frame_size])[0][0]\n",
        "    frames.append([x, y, z])\n",
        "    labels.append(label)\n",
        "  # Bring the segments into a better shape\n",
        "  frames = np.asarray(frames, dtype= np.float32).reshape(-1, frame_size, N_FEATURES)\n",
        "  labels = np.asarray(labels)\n",
        "  return frames, labels\n",
        "\n",
        "from scipy import stats\n",
        "X,y = get_frames(dataCol, frame_size, hop_size)\n",
        "\n",
        "X.shape\n",
        "y.shape\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state = 0, stratify = y)\n",
        "\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)\n",
        "\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,Dropout,BatchNormalization\n",
        "from keras.layers import Conv1D,MaxPooling1D,Flatten\n",
        "\n",
        "from keras import backend as K\n",
        "\n",
        "def recall_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "def precision_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "def f1_m(y_true, y_pred):\n",
        "    precision = precision_m(y_true, y_pred)\n",
        "    recall = recall_m(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation, Conv1D, Dropout, MaxPooling1D, Flatten, LSTM,BatchNormalization, SimpleRNN\n",
        "\n",
        "model = Sequential()\n",
        "model.add(SimpleRNN(30, input_shape=x_train[0].shape))\n",
        "model.add((Flatten()))\n",
        "model.add(Dense(3, activation='softmax'))\n",
        "model.summary()\n",
        "\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "opt = Adam(learning_rate=0.0001)\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer=opt, metrics=['accuracy',f1_m,precision_m,recall_m])\n",
        "\n",
        "from time import perf_counter\n",
        "print(\"Time Counting Started :\")\n",
        "t1_start = perf_counter()\n",
        "\n",
        "score = model.fit(x_train,y_train, epochs=30, validation_data= (x_test, y_test),batch_size=64, verbose=1)\n",
        "\n",
        "t1_stop = perf_counter()\n",
        "print(\"Elapsed time:\", t1_stop, t1_start)\n",
        "print(\"Elapsed time during the whole program in seconds:\",\n",
        "                                        t1_stop-t1_start)\n",
        "\n",
        "# Evaluate the model on the test data using `evaluate`\n",
        "print(\"Evaluate on test data\")\n",
        "results = model.evaluate(x_test, y_test)\n",
        "\n",
        "predy = np.argmax(model.predict(x_test), axis=1)\n",
        "\n",
        "from mlxtend.plotting import plot_confusion_matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "cm=confusion_matrix(y_test,predy)\n",
        "print(cm)\n",
        "print(accuracy_score(y_test,predy))\n",
        "print(\"classification_report of model1:\")\n",
        "\n",
        "mat = confusion_matrix(y_test,predy)\n",
        "\n",
        "print('Precision: %.3f' % precision_score(y_test, predy, average='micro'))\n",
        "target_names = ['class 0', 'class 1', 'class 2']\n",
        "\n",
        "print(classification_report(y_test,predy, target_names=target_names))\n",
        "\n",
        "plot_confusion_matrix(conf_mat=mat, show_normed=True)\n",
        "\n",
        "plot_confusion_matrix(conf_mat=mat, show_normed=True, show_absolute=False, colorbar=True)\n",
        "\n",
        "import matplotlib.pyplot as plt1\n",
        "\n",
        "# Plot training & validation accuracy values\n",
        "plt.figure(figsize=(4,4))\n",
        "plt.plot(score.history['accuracy'],label='Train accuracy')\n",
        "plt.plot(score.history['val_accuracy'],label = 'Validation accuracy')\n",
        "plt.xlabel('epoch no')\n",
        "plt.ylabel('acc')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(4,4))\n",
        "plt.plot(score.history['loss'],label='Train loss')\n",
        "plt.plot(score.history['val_loss'],label = 'Validation loss')\n",
        "plt.xlabel('epoch no')\n",
        "plt.ylabel('loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0yX32RveaByh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#YCS-2023_WISDM_CNN+GRU_Model_Source_Code\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Flatten, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "print(tf.__version__)\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "\n",
        "file = open('/content/drive/MyDrive/WISDM/WISDM_ar_v1.1/WISDM_ar_v1.1_raw.txt')\n",
        "lines = file.readlines()\n",
        "\n",
        "processedList = []\n",
        "\n",
        "for i, line in enumerate(lines):\n",
        "    try:\n",
        "        line = line.split(',')\n",
        "        last = line[5].split(';')[0]\n",
        "        last = last.strip()\n",
        "        if last == '':\n",
        "            break;\n",
        "        temp = [line[0], line[1], line[2], line[3], line[4], last]\n",
        "        processedList.append(temp)\n",
        "    except:\n",
        "        print('Error at line number: ', i)\n",
        "\n",
        "\n",
        "columns = ['user', 'activity', 'time', 'x', 'y', 'z']\n",
        "\n",
        "data = pd.DataFrame(data = processedList, columns = columns)\n",
        "data.head()\n",
        "\n",
        "data.shape\n",
        "\n",
        "data.isnull().sum()\n",
        "\n",
        "data['x'] = data['x'].astype('float')\n",
        "data['y'] = data['y'].astype('float')\n",
        "data['z'] = data['z'].astype('float')\n",
        "\n",
        "activities = data['activity'].value_counts().index\n",
        "activities\n",
        "\n",
        "data['activity'].value_counts()\n",
        "\n",
        "# Normalize features for training data set (values between 0 and 1)\n",
        "# Surpress warning for next 3 operation\n",
        "pd.options.mode.chained_assignment = None  # default='warn'\n",
        "data['x'] = data['x'].astype(float) / data['x'].astype(float).max()\n",
        "data['y'] = data['y'].astype(float) / data['y'].astype(float).max()\n",
        "data['z'] = data['z'].astype(float) / data['z'].astype(float).max()\n",
        "# Round numbers\n",
        "data = data.round({'x': 4, 'y': 4, 'z': 4})\n",
        "\n",
        "static = data[data['activity'].isin (['Sitting', 'Standing'])]\n",
        "static\n",
        "\n",
        "static['Encode'] = \"Static\"\n",
        "\n",
        "static\n",
        "\n",
        "gradual = data[data['activity'].isin (['Upstairs', 'Downstairs'])]\n",
        "gradual\n",
        "\n",
        "gradual['Encode'] = \"Gradual\"\n",
        "gradual\n",
        "\n",
        "dynamic = data[data['activity'].isin (['Walking', 'Jogging'])]\n",
        "dynamic\n",
        "\n",
        "dynamic['Encode'] = \"Dynamic\"\n",
        "dynamic\n",
        "\n",
        "frames = [static, gradual, dynamic\n",
        "          ]\n",
        "dataCol = pd.concat(frames)\n",
        "print(dataCol)\n",
        "dataCol['Encode'].value_counts()\n",
        "\n",
        "dataCol\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "label = LabelEncoder()\n",
        "dataCol['label'] = label.fit_transform(dataCol['Encode'])\n",
        "dataCol.head()\n",
        "\n",
        "dataCol['label'].value_counts()\n",
        "\n",
        "dataCol = dataCol.drop(['user', 'activity', 'time', 'Encode'], axis = 1).copy()\n",
        "dataCol.head()\n",
        "\n",
        "label.classes_\n",
        "\n",
        "Fs = 50\n",
        "frame_size = Fs*4 # 200\n",
        "hop_size = Fs*2 # 200\n",
        "\n",
        "import math as m\n",
        "def get_frames(data, frame_size, hop_size):\n",
        "\n",
        "  N_FEATURES = 3\n",
        "  frames = []\n",
        "  labels = []\n",
        "  for i in range(0, len(dataCol) - frame_size, hop_size):\n",
        "    x = data['x'].values[i: i + frame_size]\n",
        "    y = data['y'].values[i: i + frame_size]\n",
        "    z = data['z'].values[i: i + frame_size]\n",
        "        # Retrieve the most often used label in this segment\n",
        "    label = stats.mode(dataCol['label'][i: i + frame_size])[0][0]\n",
        "    frames.append([x, y, z])\n",
        "    labels.append(label)\n",
        "  # Bring the segments into a better shape\n",
        "  frames = np.asarray(frames, dtype= np.float32).reshape(-1, frame_size, N_FEATURES)\n",
        "  labels = np.asarray(labels)\n",
        "  return frames, labels\n",
        "\n",
        "from scipy import stats\n",
        "X,y = get_frames(dataCol, frame_size, hop_size)\n",
        "\n",
        "X.shape\n",
        "y.shape\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state = 0, stratify = y)\n",
        "\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)\n",
        "\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,Dropout,BatchNormalization\n",
        "from keras.layers import Conv1D,MaxPooling1D,Flatten\n",
        "\n",
        "from keras import backend as K\n",
        "\n",
        "def recall_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "def precision_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "def f1_m(y_true, y_pred):\n",
        "    precision = precision_m(y_true, y_pred)\n",
        "    recall = recall_m(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation, Conv1D, Dropout, MaxPooling1D, Flatten, LSTM,BatchNormalization, SimpleRNN\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv1D(filters=64, kernel_size=3, padding='same', activation='relu',input_shape=x_train[0].shape))\n",
        "model.add((GRU(30)))\n",
        "model.add((Flatten()))\n",
        "model.add(Dense(3, activation='softmax'))\n",
        "model.summary()\n",
        "\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "opt = Adam(learning_rate=0.0001)\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer=opt, metrics=['accuracy',f1_m,precision_m,recall_m])\n",
        "\n",
        "from time import perf_counter\n",
        "print(\"Time Counting Started :\")\n",
        "t1_start = perf_counter()\n",
        "\n",
        "score = model.fit(x_train,y_train, epochs=30, validation_data= (x_test, y_test),batch_size=64, verbose=1)\n",
        "\n",
        "t1_stop = perf_counter()\n",
        "print(\"Elapsed time:\", t1_stop, t1_start)\n",
        "print(\"Elapsed time during the whole program in seconds:\",\n",
        "                                        t1_stop-t1_start)\n",
        "\n",
        "# Evaluate the model on the test data using `evaluate`\n",
        "print(\"Evaluate on test data\")\n",
        "results = model.evaluate(x_test, y_test)\n",
        "\n",
        "predy = np.argmax(model.predict(x_test), axis=1)\n",
        "\n",
        "from mlxtend.plotting import plot_confusion_matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "cm=confusion_matrix(y_test,predy)\n",
        "print(cm)\n",
        "print(accuracy_score(y_test,predy))\n",
        "print(\"classification_report of model1:\")\n",
        "\n",
        "mat = confusion_matrix(y_test,predy)\n",
        "\n",
        "print('Precision: %.3f' % precision_score(y_test, predy, average='micro'))\n",
        "target_names = ['class 0', 'class 1', 'class 2']\n",
        "\n",
        "print(classification_report(y_test,predy, target_names=target_names))\n",
        "\n",
        "plot_confusion_matrix(conf_mat=mat, show_normed=True)\n",
        "\n",
        "plot_confusion_matrix(conf_mat=mat, show_normed=True, show_absolute=False, colorbar=True)\n",
        "\n",
        "import matplotlib.pyplot as plt1\n",
        "\n",
        "# Plot training & validation accuracy values\n",
        "plt.figure(figsize=(4,4))\n",
        "plt.plot(score.history['accuracy'],label='Train accuracy')\n",
        "plt.plot(score.history['val_accuracy'],label = 'Validation accuracy')\n",
        "plt.xlabel('epoch no')\n",
        "plt.ylabel('acc')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(4,4))\n",
        "plt.plot(score.history['loss'],label='Train loss')\n",
        "plt.plot(score.history['val_loss'],label = 'Validation loss')\n",
        "plt.xlabel('epoch no')\n",
        "plt.ylabel('loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-vIudhKdaU3P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#YCS-2023_WISDM_CNN+Bi-LSTM_Model_Source_Code\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Flatten, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "print(tf.__version__)\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "\n",
        "file = open('/content/drive/MyDrive/WISDM/WISDM_ar_v1.1/WISDM_ar_v1.1_raw.txt')\n",
        "lines = file.readlines()\n",
        "\n",
        "processedList = []\n",
        "\n",
        "for i, line in enumerate(lines):\n",
        "    try:\n",
        "        line = line.split(',')\n",
        "        last = line[5].split(';')[0]\n",
        "        last = last.strip()\n",
        "        if last == '':\n",
        "            break;\n",
        "        temp = [line[0], line[1], line[2], line[3], line[4], last]\n",
        "        processedList.append(temp)\n",
        "    except:\n",
        "        print('Error at line number: ', i)\n",
        "\n",
        "\n",
        "columns = ['user', 'activity', 'time', 'x', 'y', 'z']\n",
        "\n",
        "data = pd.DataFrame(data = processedList, columns = columns)\n",
        "data.head()\n",
        "\n",
        "data.shape\n",
        "\n",
        "data.isnull().sum()\n",
        "\n",
        "data['x'] = data['x'].astype('float')\n",
        "data['y'] = data['y'].astype('float')\n",
        "data['z'] = data['z'].astype('float')\n",
        "\n",
        "activities = data['activity'].value_counts().index\n",
        "activities\n",
        "\n",
        "data['activity'].value_counts()\n",
        "\n",
        "# Normalize features for training data set (values between 0 and 1)\n",
        "# Surpress warning for next 3 operation\n",
        "pd.options.mode.chained_assignment = None  # default='warn'\n",
        "data['x'] = data['x'].astype(float) / data['x'].astype(float).max()\n",
        "data['y'] = data['y'].astype(float) / data['y'].astype(float).max()\n",
        "data['z'] = data['z'].astype(float) / data['z'].astype(float).max()\n",
        "# Round numbers\n",
        "data = data.round({'x': 4, 'y': 4, 'z': 4})\n",
        "\n",
        "static = data[data['activity'].isin (['Sitting', 'Standing'])]\n",
        "static\n",
        "\n",
        "static['Encode'] = \"Static\"\n",
        "\n",
        "static\n",
        "\n",
        "gradual = data[data['activity'].isin (['Upstairs', 'Downstairs'])]\n",
        "gradual\n",
        "\n",
        "gradual['Encode'] = \"Gradual\"\n",
        "gradual\n",
        "\n",
        "dynamic = data[data['activity'].isin (['Walking', 'Jogging'])]\n",
        "dynamic\n",
        "\n",
        "dynamic['Encode'] = \"Dynamic\"\n",
        "dynamic\n",
        "\n",
        "frames = [static, gradual, dynamic\n",
        "          ]\n",
        "dataCol = pd.concat(frames)\n",
        "print(dataCol)\n",
        "dataCol['Encode'].value_counts()\n",
        "\n",
        "dataCol\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "label = LabelEncoder()\n",
        "dataCol['label'] = label.fit_transform(dataCol['Encode'])\n",
        "dataCol.head()\n",
        "\n",
        "dataCol['label'].value_counts()\n",
        "\n",
        "dataCol = dataCol.drop(['user', 'activity', 'time', 'Encode'], axis = 1).copy()\n",
        "dataCol.head()\n",
        "\n",
        "label.classes_\n",
        "\n",
        "Fs = 50\n",
        "frame_size = Fs*4 # 200\n",
        "hop_size = Fs*2 # 200\n",
        "\n",
        "import math as m\n",
        "def get_frames(data, frame_size, hop_size):\n",
        "\n",
        "  N_FEATURES = 3\n",
        "  frames = []\n",
        "  labels = []\n",
        "  for i in range(0, len(dataCol) - frame_size, hop_size):\n",
        "    x = data['x'].values[i: i + frame_size]\n",
        "    y = data['y'].values[i: i + frame_size]\n",
        "    z = data['z'].values[i: i + frame_size]\n",
        "        # Retrieve the most often used label in this segment\n",
        "    label = stats.mode(dataCol['label'][i: i + frame_size])[0][0]\n",
        "    frames.append([x, y, z])\n",
        "    labels.append(label)\n",
        "  # Bring the segments into a better shape\n",
        "  frames = np.asarray(frames, dtype= np.float32).reshape(-1, frame_size, N_FEATURES)\n",
        "  labels = np.asarray(labels)\n",
        "  return frames, labels\n",
        "\n",
        "from scipy import stats\n",
        "X,y = get_frames(dataCol, frame_size, hop_size)\n",
        "\n",
        "X.shape\n",
        "y.shape\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state = 0, stratify = y)\n",
        "\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)\n",
        "\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,Dropout,BatchNormalization\n",
        "from keras.layers import Conv1D,MaxPooling1D,Flatten\n",
        "\n",
        "from keras import backend as K\n",
        "\n",
        "def recall_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "def precision_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "def f1_m(y_true, y_pred):\n",
        "    precision = precision_m(y_true, y_pred)\n",
        "    recall = recall_m(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation, Conv1D, Dropout, MaxPooling1D, Flatten, LSTM,BatchNormalization, SimpleRNN\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv1D(filters=64, kernel_size=3, padding='same', activation='relu',input_shape=x_train[0].shape))\n",
        "model.add(Bidirectional(LSTM(30)))\n",
        "model.add((Flatten()))\n",
        "model.add(Dense(3, activation=\"softmax\"))\n",
        "model.summary()\n",
        "\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "opt = Adam(learning_rate=0.0001)\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer=opt, metrics=['accuracy',f1_m,precision_m,recall_m])\n",
        "\n",
        "from time import perf_counter\n",
        "print(\"Time Counting Started :\")\n",
        "t1_start = perf_counter()\n",
        "\n",
        "score = model.fit(x_train,y_train, epochs=30, validation_data= (x_test, y_test),batch_size=64, verbose=1)\n",
        "\n",
        "t1_stop = perf_counter()\n",
        "print(\"Elapsed time:\", t1_stop, t1_start)\n",
        "print(\"Elapsed time during the whole program in seconds:\",\n",
        "                                        t1_stop-t1_start)\n",
        "\n",
        "# Evaluate the model on the test data using `evaluate`\n",
        "print(\"Evaluate on test data\")\n",
        "results = model.evaluate(x_test, y_test)\n",
        "\n",
        "predy = np.argmax(model.predict(x_test), axis=1)\n",
        "\n",
        "from mlxtend.plotting import plot_confusion_matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "cm=confusion_matrix(y_test,predy)\n",
        "print(cm)\n",
        "print(accuracy_score(y_test,predy))\n",
        "print(\"classification_report of model1:\")\n",
        "\n",
        "mat = confusion_matrix(y_test,predy)\n",
        "\n",
        "print('Precision: %.3f' % precision_score(y_test, predy, average='micro'))\n",
        "target_names = ['class 0', 'class 1', 'class 2']\n",
        "\n",
        "print(classification_report(y_test,predy, target_names=target_names))\n",
        "\n",
        "plot_confusion_matrix(conf_mat=mat, show_normed=True)\n",
        "\n",
        "plot_confusion_matrix(conf_mat=mat, show_normed=True, show_absolute=False, colorbar=True)\n",
        "\n",
        "import matplotlib.pyplot as plt1\n",
        "\n",
        "# Plot training & validation accuracy values\n",
        "plt.figure(figsize=(4,4))\n",
        "plt.plot(score.history['accuracy'],label='Train accuracy')\n",
        "plt.plot(score.history['val_accuracy'],label = 'Validation accuracy')\n",
        "plt.xlabel('epoch no')\n",
        "plt.ylabel('acc')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(4,4))\n",
        "plt.plot(score.history['loss'],label='Train loss')\n",
        "plt.plot(score.history['val_loss'],label = 'Validation loss')\n",
        "plt.xlabel('epoch no')\n",
        "plt.ylabel('loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yje5QGUraepx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#YCS-2023_WISDM_CNN+RNN_Model_Source_Code\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Flatten, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "print(tf.__version__)\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "\n",
        "file = open('/content/drive/MyDrive/WISDM/WISDM_ar_v1.1/WISDM_ar_v1.1_raw.txt')\n",
        "lines = file.readlines()\n",
        "\n",
        "processedList = []\n",
        "\n",
        "for i, line in enumerate(lines):\n",
        "    try:\n",
        "        line = line.split(',')\n",
        "        last = line[5].split(';')[0]\n",
        "        last = last.strip()\n",
        "        if last == '':\n",
        "            break;\n",
        "        temp = [line[0], line[1], line[2], line[3], line[4], last]\n",
        "        processedList.append(temp)\n",
        "    except:\n",
        "        print('Error at line number: ', i)\n",
        "\n",
        "\n",
        "columns = ['user', 'activity', 'time', 'x', 'y', 'z']\n",
        "\n",
        "data = pd.DataFrame(data = processedList, columns = columns)\n",
        "data.head()\n",
        "\n",
        "data.shape\n",
        "\n",
        "data.isnull().sum()\n",
        "\n",
        "data['x'] = data['x'].astype('float')\n",
        "data['y'] = data['y'].astype('float')\n",
        "data['z'] = data['z'].astype('float')\n",
        "\n",
        "activities = data['activity'].value_counts().index\n",
        "activities\n",
        "\n",
        "data['activity'].value_counts()\n",
        "\n",
        "# Normalize features for training data set (values between 0 and 1)\n",
        "# Surpress warning for next 3 operation\n",
        "pd.options.mode.chained_assignment = None  # default='warn'\n",
        "data['x'] = data['x'].astype(float) / data['x'].astype(float).max()\n",
        "data['y'] = data['y'].astype(float) / data['y'].astype(float).max()\n",
        "data['z'] = data['z'].astype(float) / data['z'].astype(float).max()\n",
        "# Round numbers\n",
        "data = data.round({'x': 4, 'y': 4, 'z': 4})\n",
        "\n",
        "static = data[data['activity'].isin (['Sitting', 'Standing'])]\n",
        "static\n",
        "\n",
        "static['Encode'] = \"Static\"\n",
        "\n",
        "static\n",
        "\n",
        "gradual = data[data['activity'].isin (['Upstairs', 'Downstairs'])]\n",
        "gradual\n",
        "\n",
        "gradual['Encode'] = \"Gradual\"\n",
        "gradual\n",
        "\n",
        "dynamic = data[data['activity'].isin (['Walking', 'Jogging'])]\n",
        "dynamic\n",
        "\n",
        "dynamic['Encode'] = \"Dynamic\"\n",
        "dynamic\n",
        "\n",
        "frames = [static, gradual, dynamic\n",
        "          ]\n",
        "dataCol = pd.concat(frames)\n",
        "print(dataCol)\n",
        "dataCol['Encode'].value_counts()\n",
        "\n",
        "dataCol\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "label = LabelEncoder()\n",
        "dataCol['label'] = label.fit_transform(dataCol['Encode'])\n",
        "dataCol.head()\n",
        "\n",
        "dataCol['label'].value_counts()\n",
        "\n",
        "dataCol = dataCol.drop(['user', 'activity', 'time', 'Encode'], axis = 1).copy()\n",
        "dataCol.head()\n",
        "\n",
        "label.classes_\n",
        "\n",
        "Fs = 50\n",
        "frame_size = Fs*4 # 200\n",
        "hop_size = Fs*2 # 200\n",
        "\n",
        "import math as m\n",
        "def get_frames(data, frame_size, hop_size):\n",
        "\n",
        "  N_FEATURES = 3\n",
        "  frames = []\n",
        "  labels = []\n",
        "  for i in range(0, len(dataCol) - frame_size, hop_size):\n",
        "    x = data['x'].values[i: i + frame_size]\n",
        "    y = data['y'].values[i: i + frame_size]\n",
        "    z = data['z'].values[i: i + frame_size]\n",
        "        # Retrieve the most often used label in this segment\n",
        "    label = stats.mode(dataCol['label'][i: i + frame_size])[0][0]\n",
        "    frames.append([x, y, z])\n",
        "    labels.append(label)\n",
        "  # Bring the segments into a better shape\n",
        "  frames = np.asarray(frames, dtype= np.float32).reshape(-1, frame_size, N_FEATURES)\n",
        "  labels = np.asarray(labels)\n",
        "  return frames, labels\n",
        "\n",
        "from scipy import stats\n",
        "X,y = get_frames(dataCol, frame_size, hop_size)\n",
        "\n",
        "X.shape\n",
        "y.shape\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state = 0, stratify = y)\n",
        "\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)\n",
        "\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,Dropout,BatchNormalization\n",
        "from keras.layers import Conv1D,MaxPooling1D,Flatten\n",
        "\n",
        "from keras import backend as K\n",
        "\n",
        "def recall_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "def precision_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "def f1_m(y_true, y_pred):\n",
        "    precision = precision_m(y_true, y_pred)\n",
        "    recall = recall_m(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation, Conv1D, Dropout, MaxPooling1D, Flatten, LSTM,BatchNormalization, SimpleRNN\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv1D(filters=64, kernel_size=3, padding='same', activation='relu',input_shape=x_train[0].shape))\n",
        "model.add(SimpleRNN(32))\n",
        "model.add(Activation(\"relu\"))\n",
        "model.add((Flatten()))\n",
        "model.add(Dense(3, activation=\"softmax\"))\n",
        "model.summary()\n",
        "\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "opt = Adam(learning_rate=0.0001)\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer=opt, metrics=['accuracy',f1_m,precision_m,recall_m])\n",
        "\n",
        "from time import perf_counter\n",
        "print(\"Time Counting Started :\")\n",
        "t1_start = perf_counter()\n",
        "\n",
        "score = model.fit(x_train,y_train, epochs=30, validation_data= (x_test, y_test),batch_size=64, verbose=1)\n",
        "\n",
        "t1_stop = perf_counter()\n",
        "print(\"Elapsed time:\", t1_stop, t1_start)\n",
        "print(\"Elapsed time during the whole program in seconds:\",\n",
        "                                        t1_stop-t1_start)\n",
        "\n",
        "# Evaluate the model on the test data using `evaluate`\n",
        "print(\"Evaluate on test data\")\n",
        "results = model.evaluate(x_test, y_test)\n",
        "\n",
        "predy = np.argmax(model.predict(x_test), axis=1)\n",
        "\n",
        "from mlxtend.plotting import plot_confusion_matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "cm=confusion_matrix(y_test,predy)\n",
        "print(cm)\n",
        "print(accuracy_score(y_test,predy))\n",
        "print(\"classification_report of model1:\")\n",
        "\n",
        "mat = confusion_matrix(y_test,predy)\n",
        "\n",
        "print('Precision: %.3f' % precision_score(y_test, predy, average='micro'))\n",
        "target_names = ['class 0', 'class 1', 'class 2']\n",
        "\n",
        "print(classification_report(y_test,predy, target_names=target_names))\n",
        "\n",
        "plot_confusion_matrix(conf_mat=mat, show_normed=True)\n",
        "\n",
        "plot_confusion_matrix(conf_mat=mat, show_normed=True, show_absolute=False, colorbar=True)\n",
        "\n",
        "import matplotlib.pyplot as plt1\n",
        "\n",
        "# Plot training & validation accuracy values\n",
        "plt.figure(figsize=(4,4))\n",
        "plt.plot(score.history['accuracy'],label='Train accuracy')\n",
        "plt.plot(score.history['val_accuracy'],label = 'Validation accuracy')\n",
        "plt.xlabel('epoch no')\n",
        "plt.ylabel('acc')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(4,4))\n",
        "plt.plot(score.history['loss'],label='Train loss')\n",
        "plt.plot(score.history['val_loss'],label = 'Validation loss')\n",
        "plt.xlabel('epoch no')\n",
        "plt.ylabel('loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vjk70SiHaoCD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#YCS-2023_WISDM_GRU_Model_Source_Code\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Flatten, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "print(tf.__version__)\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "\n",
        "file = open('/content/drive/MyDrive/WISDM/WISDM_ar_v1.1/WISDM_ar_v1.1_raw.txt')\n",
        "lines = file.readlines()\n",
        "\n",
        "processedList = []\n",
        "\n",
        "for i, line in enumerate(lines):\n",
        "    try:\n",
        "        line = line.split(',')\n",
        "        last = line[5].split(';')[0]\n",
        "        last = last.strip()\n",
        "        if last == '':\n",
        "            break;\n",
        "        temp = [line[0], line[1], line[2], line[3], line[4], last]\n",
        "        processedList.append(temp)\n",
        "    except:\n",
        "        print('Error at line number: ', i)\n",
        "\n",
        "\n",
        "columns = ['user', 'activity', 'time', 'x', 'y', 'z']\n",
        "\n",
        "data = pd.DataFrame(data = processedList, columns = columns)\n",
        "data.head()\n",
        "\n",
        "data.shape\n",
        "\n",
        "data.isnull().sum()\n",
        "\n",
        "data['x'] = data['x'].astype('float')\n",
        "data['y'] = data['y'].astype('float')\n",
        "data['z'] = data['z'].astype('float')\n",
        "\n",
        "activities = data['activity'].value_counts().index\n",
        "activities\n",
        "\n",
        "data['activity'].value_counts()\n",
        "\n",
        "# Normalize features for training data set (values between 0 and 1)\n",
        "# Surpress warning for next 3 operation\n",
        "pd.options.mode.chained_assignment = None  # default='warn'\n",
        "data['x'] = data['x'].astype(float) / data['x'].astype(float).max()\n",
        "data['y'] = data['y'].astype(float) / data['y'].astype(float).max()\n",
        "data['z'] = data['z'].astype(float) / data['z'].astype(float).max()\n",
        "# Round numbers\n",
        "data = data.round({'x': 4, 'y': 4, 'z': 4})\n",
        "\n",
        "static = data[data['activity'].isin (['Sitting', 'Standing'])]\n",
        "static\n",
        "\n",
        "static['Encode'] = \"Static\"\n",
        "\n",
        "static\n",
        "\n",
        "gradual = data[data['activity'].isin (['Upstairs', 'Downstairs'])]\n",
        "gradual\n",
        "\n",
        "gradual['Encode'] = \"Gradual\"\n",
        "gradual\n",
        "\n",
        "dynamic = data[data['activity'].isin (['Walking', 'Jogging'])]\n",
        "dynamic\n",
        "\n",
        "dynamic['Encode'] = \"Dynamic\"\n",
        "dynamic\n",
        "\n",
        "frames = [static, gradual, dynamic\n",
        "          ]\n",
        "dataCol = pd.concat(frames)\n",
        "print(dataCol)\n",
        "dataCol['Encode'].value_counts()\n",
        "\n",
        "dataCol\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "label = LabelEncoder()\n",
        "dataCol['label'] = label.fit_transform(dataCol['Encode'])\n",
        "dataCol.head()\n",
        "\n",
        "dataCol['label'].value_counts()\n",
        "\n",
        "dataCol = dataCol.drop(['user', 'activity', 'time', 'Encode'], axis = 1).copy()\n",
        "dataCol.head()\n",
        "\n",
        "label.classes_\n",
        "\n",
        "Fs = 50\n",
        "frame_size = Fs*4 # 200\n",
        "hop_size = Fs*2 # 200\n",
        "\n",
        "import math as m\n",
        "def get_frames(data, frame_size, hop_size):\n",
        "\n",
        "  N_FEATURES = 3\n",
        "  frames = []\n",
        "  labels = []\n",
        "  for i in range(0, len(dataCol) - frame_size, hop_size):\n",
        "    x = data['x'].values[i: i + frame_size]\n",
        "    y = data['y'].values[i: i + frame_size]\n",
        "    z = data['z'].values[i: i + frame_size]\n",
        "        # Retrieve the most often used label in this segment\n",
        "    label = stats.mode(dataCol['label'][i: i + frame_size])[0][0]\n",
        "    frames.append([x, y, z])\n",
        "    labels.append(label)\n",
        "  # Bring the segments into a better shape\n",
        "  frames = np.asarray(frames, dtype= np.float32).reshape(-1, frame_size, N_FEATURES)\n",
        "  labels = np.asarray(labels)\n",
        "  return frames, labels\n",
        "\n",
        "from scipy import stats\n",
        "X,y = get_frames(dataCol, frame_size, hop_size)\n",
        "\n",
        "X.shape\n",
        "y.shape\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state = 0, stratify = y)\n",
        "\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)\n",
        "\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,Dropout,BatchNormalization\n",
        "from keras.layers import Conv1D,MaxPooling1D,Flatten\n",
        "\n",
        "from keras import backend as K\n",
        "\n",
        "def recall_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "def precision_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "def f1_m(y_true, y_pred):\n",
        "    precision = precision_m(y_true, y_pred)\n",
        "    recall = recall_m(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation, Conv1D, Dropout, MaxPooling1D, Flatten, LSTM,BatchNormalization, SimpleRNN\n",
        "\n",
        "model = Sequential()\n",
        "model.add(GRU(30, return_sequences=True, input_shape=x_train[0].shape))\n",
        "model.add((GRU(30)))\n",
        "model.add((Flatten()))\n",
        "model.add(Dense(3, activation='softmax'))\n",
        "model.summary()\n",
        "\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "opt = Adam(learning_rate=0.0001)\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer=opt, metrics=['accuracy',f1_m,precision_m,recall_m])\n",
        "\n",
        "from time import perf_counter\n",
        "print(\"Time Counting Started :\")\n",
        "t1_start = perf_counter()\n",
        "\n",
        "score = model.fit(x_train,y_train, epochs=30, validation_data= (x_test, y_test),batch_size=64, verbose=1)\n",
        "\n",
        "t1_stop = perf_counter()\n",
        "print(\"Elapsed time:\", t1_stop, t1_start)\n",
        "print(\"Elapsed time during the whole program in seconds:\",\n",
        "                                        t1_stop-t1_start)\n",
        "\n",
        "# Evaluate the model on the test data using `evaluate`\n",
        "print(\"Evaluate on test data\")\n",
        "results = model.evaluate(x_test, y_test)\n",
        "\n",
        "predy = np.argmax(model.predict(x_test), axis=1)\n",
        "\n",
        "from mlxtend.plotting import plot_confusion_matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "cm=confusion_matrix(y_test,predy)\n",
        "print(cm)\n",
        "print(accuracy_score(y_test,predy))\n",
        "print(\"classification_report of model1:\")\n",
        "\n",
        "mat = confusion_matrix(y_test,predy)\n",
        "\n",
        "print('Precision: %.3f' % precision_score(y_test, predy, average='micro'))\n",
        "target_names = ['class 0', 'class 1', 'class 2']\n",
        "\n",
        "print(classification_report(y_test,predy, target_names=target_names))\n",
        "\n",
        "plot_confusion_matrix(conf_mat=mat, show_normed=True)\n",
        "\n",
        "plot_confusion_matrix(conf_mat=mat, show_normed=True, show_absolute=False, colorbar=True)\n",
        "\n",
        "import matplotlib.pyplot as plt1\n",
        "\n",
        "# Plot training & validation accuracy values\n",
        "plt.figure(figsize=(4,4))\n",
        "plt.plot(score.history['accuracy'],label='Train accuracy')\n",
        "plt.plot(score.history['val_accuracy'],label = 'Validation accuracy')\n",
        "plt.xlabel('epoch no')\n",
        "plt.ylabel('acc')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(4,4))\n",
        "plt.plot(score.history['loss'],label='Train loss')\n",
        "plt.plot(score.history['val_loss'],label = 'Validation loss')\n",
        "plt.xlabel('epoch no')\n",
        "plt.ylabel('loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CcgWldNua0tJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#YCS-2023_WISDM_Bi-LSTM_Model_Source_Code\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Flatten, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "print(tf.__version__)\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "\n",
        "file = open('/content/drive/MyDrive/WISDM/WISDM_ar_v1.1/WISDM_ar_v1.1_raw.txt')\n",
        "lines = file.readlines()\n",
        "\n",
        "processedList = []\n",
        "\n",
        "for i, line in enumerate(lines):\n",
        "    try:\n",
        "        line = line.split(',')\n",
        "        last = line[5].split(';')[0]\n",
        "        last = last.strip()\n",
        "        if last == '':\n",
        "            break;\n",
        "        temp = [line[0], line[1], line[2], line[3], line[4], last]\n",
        "        processedList.append(temp)\n",
        "    except:\n",
        "        print('Error at line number: ', i)\n",
        "\n",
        "\n",
        "columns = ['user', 'activity', 'time', 'x', 'y', 'z']\n",
        "\n",
        "data = pd.DataFrame(data = processedList, columns = columns)\n",
        "data.head()\n",
        "\n",
        "data.shape\n",
        "\n",
        "data.isnull().sum()\n",
        "\n",
        "data['x'] = data['x'].astype('float')\n",
        "data['y'] = data['y'].astype('float')\n",
        "data['z'] = data['z'].astype('float')\n",
        "\n",
        "activities = data['activity'].value_counts().index\n",
        "activities\n",
        "\n",
        "data['activity'].value_counts()\n",
        "\n",
        "# Normalize features for training data set (values between 0 and 1)\n",
        "# Surpress warning for next 3 operation\n",
        "pd.options.mode.chained_assignment = None  # default='warn'\n",
        "data['x'] = data['x'].astype(float) / data['x'].astype(float).max()\n",
        "data['y'] = data['y'].astype(float) / data['y'].astype(float).max()\n",
        "data['z'] = data['z'].astype(float) / data['z'].astype(float).max()\n",
        "# Round numbers\n",
        "data = data.round({'x': 4, 'y': 4, 'z': 4})\n",
        "\n",
        "static = data[data['activity'].isin (['Sitting', 'Standing'])]\n",
        "static\n",
        "\n",
        "static['Encode'] = \"Static\"\n",
        "\n",
        "static\n",
        "\n",
        "gradual = data[data['activity'].isin (['Upstairs', 'Downstairs'])]\n",
        "gradual\n",
        "\n",
        "gradual['Encode'] = \"Gradual\"\n",
        "gradual\n",
        "\n",
        "dynamic = data[data['activity'].isin (['Walking', 'Jogging'])]\n",
        "dynamic\n",
        "\n",
        "dynamic['Encode'] = \"Dynamic\"\n",
        "dynamic\n",
        "\n",
        "frames = [static, gradual, dynamic\n",
        "          ]\n",
        "dataCol = pd.concat(frames)\n",
        "print(dataCol)\n",
        "dataCol['Encode'].value_counts()\n",
        "\n",
        "dataCol\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "label = LabelEncoder()\n",
        "dataCol['label'] = label.fit_transform(dataCol['Encode'])\n",
        "dataCol.head()\n",
        "\n",
        "dataCol['label'].value_counts()\n",
        "\n",
        "dataCol = dataCol.drop(['user', 'activity', 'time', 'Encode'], axis = 1).copy()\n",
        "dataCol.head()\n",
        "\n",
        "label.classes_\n",
        "\n",
        "Fs = 50\n",
        "frame_size = Fs*4 # 200\n",
        "hop_size = Fs*2 # 200\n",
        "\n",
        "import math as m\n",
        "def get_frames(data, frame_size, hop_size):\n",
        "\n",
        "  N_FEATURES = 3\n",
        "  frames = []\n",
        "  labels = []\n",
        "  for i in range(0, len(dataCol) - frame_size, hop_size):\n",
        "    x = data['x'].values[i: i + frame_size]\n",
        "    y = data['y'].values[i: i + frame_size]\n",
        "    z = data['z'].values[i: i + frame_size]\n",
        "        # Retrieve the most often used label in this segment\n",
        "    label = stats.mode(dataCol['label'][i: i + frame_size])[0][0]\n",
        "    frames.append([x, y, z])\n",
        "    labels.append(label)\n",
        "  # Bring the segments into a better shape\n",
        "  frames = np.asarray(frames, dtype= np.float32).reshape(-1, frame_size, N_FEATURES)\n",
        "  labels = np.asarray(labels)\n",
        "  return frames, labels\n",
        "\n",
        "from scipy import stats\n",
        "X,y = get_frames(dataCol, frame_size, hop_size)\n",
        "\n",
        "X.shape\n",
        "y.shape\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state = 0, stratify = y)\n",
        "\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)\n",
        "\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,Dropout,BatchNormalization\n",
        "from keras.layers import Conv1D,MaxPooling1D,Flatten\n",
        "\n",
        "from keras import backend as K\n",
        "\n",
        "def recall_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "def precision_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "def f1_m(y_true, y_pred):\n",
        "    precision = precision_m(y_true, y_pred)\n",
        "    recall = recall_m(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation, Conv1D, Dropout, MaxPooling1D, Flatten, LSTM,BatchNormalization, SimpleRNN\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Bidirectional(LSTM(30, return_sequences=True), input_shape=x_train[0].shape))\n",
        "model.add(Bidirectional(LSTM(30)))\n",
        "model.add((Flatten()))\n",
        "model.add(Dense(3, activation='softmax'))\n",
        "model.summary()\n",
        "\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "opt = Adam(learning_rate=0.0001)\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer=opt, metrics=['accuracy',f1_m,precision_m,recall_m])\n",
        "\n",
        "from time import perf_counter\n",
        "print(\"Time Counting Started :\")\n",
        "t1_start = perf_counter()\n",
        "\n",
        "score = model.fit(x_train,y_train, epochs=30, validation_data= (x_test, y_test),batch_size=64, verbose=1)\n",
        "\n",
        "t1_stop = perf_counter()\n",
        "print(\"Elapsed time:\", t1_stop, t1_start)\n",
        "print(\"Elapsed time during the whole program in seconds:\",\n",
        "                                        t1_stop-t1_start)\n",
        "\n",
        "# Evaluate the model on the test data using `evaluate`\n",
        "print(\"Evaluate on test data\")\n",
        "results = model.evaluate(x_test, y_test)\n",
        "\n",
        "predy = np.argmax(model.predict(x_test), axis=1)\n",
        "\n",
        "from mlxtend.plotting import plot_confusion_matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "cm=confusion_matrix(y_test,predy)\n",
        "print(cm)\n",
        "print(accuracy_score(y_test,predy))\n",
        "print(\"classification_report of model1:\")\n",
        "\n",
        "mat = confusion_matrix(y_test,predy)\n",
        "\n",
        "print('Precision: %.3f' % precision_score(y_test, predy, average='micro'))\n",
        "target_names = ['class 0', 'class 1', 'class 2']\n",
        "\n",
        "print(classification_report(y_test,predy, target_names=target_names))\n",
        "\n",
        "plot_confusion_matrix(conf_mat=mat, show_normed=True)\n",
        "\n",
        "plot_confusion_matrix(conf_mat=mat, show_normed=True, show_absolute=False, colorbar=True)\n",
        "\n",
        "import matplotlib.pyplot as plt1\n",
        "\n",
        "# Plot training & validation accuracy values\n",
        "plt.figure(figsize=(4,4))\n",
        "plt.plot(score.history['accuracy'],label='Train accuracy')\n",
        "plt.plot(score.history['val_accuracy'],label = 'Validation accuracy')\n",
        "plt.xlabel('epoch no')\n",
        "plt.ylabel('acc')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(4,4))\n",
        "plt.plot(score.history['loss'],label='Train loss')\n",
        "plt.plot(score.history['val_loss'],label = 'Validation loss')\n",
        "plt.xlabel('epoch no')\n",
        "plt.ylabel('loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Fo2j_DQVa-Ix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#YCS-2023_WISDM_LSTM_Model_Source_Code\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Flatten, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "print(tf.__version__)\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "\n",
        "file = open('/content/drive/MyDrive/WISDM/WISDM_ar_v1.1/WISDM_ar_v1.1_raw.txt')\n",
        "lines = file.readlines()\n",
        "\n",
        "processedList = []\n",
        "\n",
        "for i, line in enumerate(lines):\n",
        "    try:\n",
        "        line = line.split(',')\n",
        "        last = line[5].split(';')[0]\n",
        "        last = last.strip()\n",
        "        if last == '':\n",
        "            break;\n",
        "        temp = [line[0], line[1], line[2], line[3], line[4], last]\n",
        "        processedList.append(temp)\n",
        "    except:\n",
        "        print('Error at line number: ', i)\n",
        "\n",
        "\n",
        "columns = ['user', 'activity', 'time', 'x', 'y', 'z']\n",
        "\n",
        "data = pd.DataFrame(data = processedList, columns = columns)\n",
        "data.head()\n",
        "\n",
        "data.shape\n",
        "\n",
        "data.isnull().sum()\n",
        "\n",
        "data['x'] = data['x'].astype('float')\n",
        "data['y'] = data['y'].astype('float')\n",
        "data['z'] = data['z'].astype('float')\n",
        "\n",
        "activities = data['activity'].value_counts().index\n",
        "activities\n",
        "\n",
        "data['activity'].value_counts()\n",
        "\n",
        "# Normalize features for training data set (values between 0 and 1)\n",
        "# Surpress warning for next 3 operation\n",
        "pd.options.mode.chained_assignment = None  # default='warn'\n",
        "data['x'] = data['x'].astype(float) / data['x'].astype(float).max()\n",
        "data['y'] = data['y'].astype(float) / data['y'].astype(float).max()\n",
        "data['z'] = data['z'].astype(float) / data['z'].astype(float).max()\n",
        "# Round numbers\n",
        "data = data.round({'x': 4, 'y': 4, 'z': 4})\n",
        "\n",
        "static = data[data['activity'].isin (['Sitting', 'Standing'])]\n",
        "static\n",
        "\n",
        "static['Encode'] = \"Static\"\n",
        "\n",
        "static\n",
        "\n",
        "gradual = data[data['activity'].isin (['Upstairs', 'Downstairs'])]\n",
        "gradual\n",
        "\n",
        "gradual['Encode'] = \"Gradual\"\n",
        "gradual\n",
        "\n",
        "dynamic = data[data['activity'].isin (['Walking', 'Jogging'])]\n",
        "dynamic\n",
        "\n",
        "dynamic['Encode'] = \"Dynamic\"\n",
        "dynamic\n",
        "\n",
        "frames = [static, gradual, dynamic\n",
        "          ]\n",
        "dataCol = pd.concat(frames)\n",
        "print(dataCol)\n",
        "dataCol['Encode'].value_counts()\n",
        "\n",
        "dataCol\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "label = LabelEncoder()\n",
        "dataCol['label'] = label.fit_transform(dataCol['Encode'])\n",
        "dataCol.head()\n",
        "\n",
        "dataCol['label'].value_counts()\n",
        "\n",
        "dataCol = dataCol.drop(['user', 'activity', 'time', 'Encode'], axis = 1).copy()\n",
        "dataCol.head()\n",
        "\n",
        "label.classes_\n",
        "\n",
        "Fs = 50\n",
        "frame_size = Fs*4 # 200\n",
        "hop_size = Fs*2 # 200\n",
        "\n",
        "import math as m\n",
        "def get_frames(data, frame_size, hop_size):\n",
        "\n",
        "  N_FEATURES = 3\n",
        "  frames = []\n",
        "  labels = []\n",
        "  for i in range(0, len(dataCol) - frame_size, hop_size):\n",
        "    x = data['x'].values[i: i + frame_size]\n",
        "    y = data['y'].values[i: i + frame_size]\n",
        "    z = data['z'].values[i: i + frame_size]\n",
        "        # Retrieve the most often used label in this segment\n",
        "    label = stats.mode(dataCol['label'][i: i + frame_size])[0][0]\n",
        "    frames.append([x, y, z])\n",
        "    labels.append(label)\n",
        "  # Bring the segments into a better shape\n",
        "  frames = np.asarray(frames, dtype= np.float32).reshape(-1, frame_size, N_FEATURES)\n",
        "  labels = np.asarray(labels)\n",
        "  return frames, labels\n",
        "\n",
        "from scipy import stats\n",
        "X,y = get_frames(dataCol, frame_size, hop_size)\n",
        "\n",
        "X.shape\n",
        "y.shape\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state = 0, stratify = y)\n",
        "\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)\n",
        "\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,Dropout,BatchNormalization\n",
        "from keras.layers import Conv1D,MaxPooling1D,Flatten\n",
        "\n",
        "from keras import backend as K\n",
        "\n",
        "def recall_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "def precision_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "def f1_m(y_true, y_pred):\n",
        "    precision = precision_m(y_true, y_pred)\n",
        "    recall = recall_m(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation, Conv1D, Dropout, MaxPooling1D, Flatten, LSTM,BatchNormalization, SimpleRNN\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(30, return_sequences=True, input_shape=x_train[0].shape))\n",
        "model.add((LSTM(30)))\n",
        "model.add((Flatten()))\n",
        "model.add(Dense(3, activation='softmax'))\n",
        "model.summary()\n",
        "\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "opt = Adam(learning_rate=0.0001)\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer=opt, metrics=['accuracy',f1_m,precision_m,recall_m])\n",
        "\n",
        "from time import perf_counter\n",
        "print(\"Time Counting Started :\")\n",
        "t1_start = perf_counter()\n",
        "\n",
        "score = model.fit(x_train,y_train, epochs=30, validation_data= (x_test, y_test),batch_size=64, verbose=1)\n",
        "\n",
        "t1_stop = perf_counter()\n",
        "print(\"Elapsed time:\", t1_stop, t1_start)\n",
        "print(\"Elapsed time during the whole program in seconds:\",\n",
        "                                        t1_stop-t1_start)\n",
        "\n",
        "# Evaluate the model on the test data using `evaluate`\n",
        "print(\"Evaluate on test data\")\n",
        "results = model.evaluate(x_test, y_test)\n",
        "\n",
        "predy = np.argmax(model.predict(x_test), axis=1)\n",
        "\n",
        "from mlxtend.plotting import plot_confusion_matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "cm=confusion_matrix(y_test,predy)\n",
        "print(cm)\n",
        "print(accuracy_score(y_test,predy))\n",
        "print(\"classification_report of model1:\")\n",
        "\n",
        "mat = confusion_matrix(y_test,predy)\n",
        "\n",
        "print('Precision: %.3f' % precision_score(y_test, predy, average='micro'))\n",
        "target_names = ['class 0', 'class 1', 'class 2']\n",
        "\n",
        "print(classification_report(y_test,predy, target_names=target_names))\n",
        "\n",
        "plot_confusion_matrix(conf_mat=mat, show_normed=True)\n",
        "\n",
        "plot_confusion_matrix(conf_mat=mat, show_normed=True, show_absolute=False, colorbar=True)\n",
        "\n",
        "import matplotlib.pyplot as plt1\n",
        "\n",
        "# Plot training & validation accuracy values\n",
        "plt.figure(figsize=(4,4))\n",
        "plt.plot(score.history['accuracy'],label='Train accuracy')\n",
        "plt.plot(score.history['val_accuracy'],label = 'Validation accuracy')\n",
        "plt.xlabel('epoch no')\n",
        "plt.ylabel('acc')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(4,4))\n",
        "plt.plot(score.history['loss'],label='Train loss')\n",
        "plt.plot(score.history['val_loss'],label = 'Validation loss')\n",
        "plt.xlabel('epoch no')\n",
        "plt.ylabel('loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Np6KNZ2FbH0u"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}